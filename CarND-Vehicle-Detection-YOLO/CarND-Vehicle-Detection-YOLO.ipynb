{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vehicle Detection using YOLO Net\n",
    "\n",
    "The goal here is to reproduce the task achieved using HOG features and SVM but using a special type of deep neural net called a YOLO network.\n",
    "\n",
    "In this case YOLO stands for You Only Look Once, and has the great advantage of not requiring any preprocessing of the image. The all image is fed to the network and it is able to detect multiple objects at any location in the image.\n",
    "\n",
    "The idea was proposed in an article by Redmon [1] and later improved by the same authors [2].\n",
    "The biggest advantages of YOLO w.r.t. to other detection algorithms (including other based on deep learning) is the speed at which it can make prediction, which makes it a good candidate for real time processing on small devices.\n",
    "\n",
    "The architecture we will implement here is slightly different than the one described in the paper, having only 9 convolutional layers, but it will be significantly faster with a relatively good accuracy.\n",
    "\n",
    "Training such a network (even the TinyYOLO version) is quite a heavy task. To avoid, this, we will simply reuse the weights of a TinyYOLO network trained to recognize 20 differents objects, among which cars. This is a bit an overkill, since we don't need our car to recognize horses, but it will still be much more efficient than SVM's on subregions. The weights can be downloaded [here]:https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "The network will be implemented in PyTorch, for no particular reason. I justa wanted to try pytorch to give it a try.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "from scipy.misc import imresize, imsave\n",
    "\n",
    "def sigmoid(x):\n",
    "    # compute sigmoid of float input\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "\n",
    "Next we define the architecture of the tinyYOLO network as folows :\n",
    "* input : 416x416x3 image\n",
    "* Conv1 : conv layer with 16 channels, filter of size 3, stride of 1 and zero padding of 1 followed by max pooling with filter size 2 and stride of 2.\n",
    "* Conv2 : conv layer with 32 channels, filter of size 3, stride of 1 and zero padding of 1 followed by max pooling with filter size 2 and stride of 2.\n",
    "* Conv3 : conv layer with 64 channels, filter of size 3, stride of 1 and zero padding of 1 followed by max pooling with filter size 2 and stride of 2.\n",
    "* Conv4 : conv layer with 128 channels, filter of size 3, stride of 1 and zero padding of 1 followed by max pooling with filter size 2 and stride of 2.\n",
    "* Conv5 : conv layer with 256 channels, filter of size 3, stride of 1 and zero padding of 1 followed by max pooling with filter size 2 and stride of 2.\n",
    "* Conv6 : conv layer with 512 channels, filter of size 3, stride of 1 and zero padding of 1 followed by a specific max xpooling layer described later.\n",
    "* Conv7 : conv layer with 1024 channels, filter of size 3, stride of 1 and zero padding of 1.\n",
    "* Conv8 : conv layer with 1024 channels, filter of size 3, stride of 1 and zero padding of 1.\n",
    "* Output : conv layer with 125 channels.\n",
    "\n",
    "Each layer, except for the output, is followed by a batch norm layer and uses leakyRELU as an activation function.\n",
    "\n",
    "The layer conv 6 needs a specific maxpool layer. If you compute shrinkage induced by previous pooling layers, you will find that the conv6 layer needs a padding of 0.5 which doesn't fit. The original model is implemented in C in the darknet package. There seems to be a tweak in the use of the maxpooling function : the function accepts an int for the padding parameter, but is passed a float (0.5). In the end, I don't know what's happening in darknet but the solution here was to use a specific pooling layer which pads a zero on the right and bottom of the image, and nothing at the left and top (kind of an asymmetric padding). This is implemented in the class MaxPoolStride1.\n",
    "\n",
    "Note that we only have convolutional layers and no fully connected layers, which keeps the number of of weights minimum.\n",
    "\n",
    "If you work out the maths, you can find that the output as a size of 125x13x13. The 125 channels provides the information on possibly detected objects : For each of the 13x13 cells, there are 5 possible boxes detected. Detection is charcterized by 25 numbers : the first five correspond to box center x, box center y, box width, box height, and logit of detection confidence. The last 20 are class confidence logits for each of the 20 classes considered in this network. \n",
    "\n",
    "The detection works as follows. Each one of the 13x13 grid cell is able to fit 5 different boxes. The 5 boxes shape are defined by priors (width and height), and the network makes a regression to adjust the parameters of the box : offset w.r.t to the grid cell center and dimensions. Note that the network only perform regressions, including on the binary confidence score and on the softmax for object detection. \n",
    "\n",
    "The class TinyYoloNet also includes methods to load the weights from a pretrained model. The model we use is based on a darknet architecture. We need two different methods, because darknet defines convolutional layers followed by batch norm as one layer. The parameters are stored as batch norm parameters followed by filter parameters. Thus we have one method for conv layer followed by batch norm, and then one method for conv layer only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolStride1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaxPoolStride1, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.pad(x, (0, 1, 0, 1), mode='replicate'), 2, stride=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyYoloNet(nn.Module):\n",
    "    def __init__(self, weight_file = None):\n",
    "        super(TinyYoloNet, self).__init__()\n",
    "        # The pretrained model we use has 20 classes\n",
    "        self.num_classes = 20\n",
    "        self.box_priors = [1.08, 1.19, 3.42, 4.41, 6.63, 11.38, 9.42, 5.11, 16.62, 10.52]\n",
    "        self.num_box_priors = int(len(self.box_priors) / 2)\n",
    "        self.num_output = int((5 + self.num_classes) * self.num_box_priors)\n",
    "        # definition of the network\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            # conv1\n",
    "            ('conv1', nn.Conv2d(3, 16, 3, 1, 1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(16)),\n",
    "            ('leaky1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2)),\n",
    "\n",
    "            # conv2\n",
    "            ('conv2', nn.Conv2d(16, 32, 3, 1, 1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(32)),\n",
    "            ('leaky2', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(2, 2)),\n",
    "\n",
    "            # conv3\n",
    "            ('conv3', nn.Conv2d(32, 64, 3, 1, 1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(64)),\n",
    "            ('leaky3', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2)),\n",
    "\n",
    "            # conv4\n",
    "            ('conv4', nn.Conv2d(64, 128, 3, 1, 1, bias=False)),\n",
    "            ('bn4', nn.BatchNorm2d(128)),\n",
    "            ('leaky4', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('pool4', nn.MaxPool2d(2, 2)),\n",
    "\n",
    "            # conv5\n",
    "            ('conv5', nn.Conv2d(128, 256, 3, 1, 1, bias=False)),\n",
    "            ('bn5', nn.BatchNorm2d(256)),\n",
    "            ('leaky5', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('pool5', nn.MaxPool2d(2, 2)),\n",
    "\n",
    "            # conv6\n",
    "            ('conv6', nn.Conv2d(256, 512, 3, 1, 1, bias=False)),\n",
    "            ('bn6', nn.BatchNorm2d(512)),\n",
    "            ('leaky6', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('pool6', MaxPoolStride1()),\n",
    "\n",
    "            # conv7\n",
    "            ('conv7', nn.Conv2d(512, 1024, 3, 1, 1, bias=False)),\n",
    "            ('bn7', nn.BatchNorm2d(1024)),\n",
    "            ('leaky7', nn.LeakyReLU(0.1, inplace=True)),\n",
    "\n",
    "            # conv8\n",
    "            ('conv8', nn.Conv2d(1024, 1024, 3, 1, 1, bias=False)),\n",
    "            ('bn8', nn.BatchNorm2d(1024)),\n",
    "            ('leaky8', nn.LeakyReLU(0.1, inplace=True)),\n",
    "\n",
    "            # output\n",
    "            ('output', nn.Conv2d(1024, self.num_output, 1, 1, 0)),\n",
    "        ]))\n",
    "        # cast all parameters to float datatype\n",
    "        self.cnn.float()\n",
    "        # set the network in evaluation mode\n",
    "        self.cnn.eval()\n",
    "        self.weight_file = weight_file\n",
    "        if weight_file:\n",
    "            # buffer to the weight file\n",
    "            self.buf = np.fromfile(self.weight_file, dtype=np.float32)\n",
    "            # first four float number in weight file are not weights\n",
    "            self.pos = 4\n",
    "            self.load_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return x\n",
    "\n",
    "    def load_weights(self):\n",
    "        # Conv1\n",
    "        self.load_conv_bn(self.cnn[0], self.cnn[1])\n",
    "        # Conv2\n",
    "        self.load_conv_bn(self.cnn[4], self.cnn[5])\n",
    "        # Conv3\n",
    "        self.load_conv_bn(self.cnn[8], self.cnn[9])\n",
    "        # Conv4\n",
    "        self.load_conv_bn(self.cnn[12], self.cnn[13])\n",
    "        # Conv5\n",
    "        self.load_conv_bn(self.cnn[16], self.cnn[17])\n",
    "        # Conv6\n",
    "        self.load_conv_bn(self.cnn[20], self.cnn[21])\n",
    "        # Conv7\n",
    "        self.load_conv_bn(self.cnn[24], self.cnn[25])\n",
    "        # Conv8\n",
    "        self.load_conv_bn(self.cnn[27], self.cnn[28])\n",
    "        # output\n",
    "        self.load_conv(self.cnn[30])\n",
    "\n",
    "        return\n",
    "\n",
    "    def load_conv_bn(self, conv_model, bn_model):\n",
    "        num_w = conv_model.weight.numel()\n",
    "        num_b = bn_model.bias.numel()\n",
    "        bn_model.bias.data.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_b]))\n",
    "        self.pos += num_b\n",
    "        bn_model.weight.data.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_b]))\n",
    "        self.pos += num_b\n",
    "        bn_model.running_mean.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_b]))\n",
    "        self.pos += num_b\n",
    "        bn_model.running_var.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_b]))\n",
    "        self.pos += num_b\n",
    "        conv_model.weight.data.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_w]).view_as(conv_model.weight.data))\n",
    "        self.pos += num_w\n",
    "\n",
    "        return\n",
    "\n",
    "    def load_conv(self, conv_model):\n",
    "        num_w = conv_model.weight.numel()\n",
    "        num_b = conv_model.bias.numel()\n",
    "        conv_model.bias.data.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_b]))\n",
    "        self.pos += num_b\n",
    "        conv_model.weight.data.copy_(torch.from_numpy(self.buf[self.pos:self.pos + num_w]).view_as(conv_model.weight.data))\n",
    "        self.pos += num_w\n",
    "\n",
    "        return\n",
    "\n",
    "    def detect(self,img):\n",
    "        # convert image to torch tensor\n",
    "        img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "        img = torch.autograd.Variable(img)\n",
    "        # Make forward pass for image, and keep values as output\n",
    "        output = self.cnn(img).data\n",
    "\n",
    "\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_boxes(output, conf_thresh, box_priors, num_box_priors):\n",
    "\n",
    "    batch = output.size(0)\n",
    "    h = output.size(2)\n",
    "    w = output.size(3)\n",
    "\n",
    "    all_boxes = []\n",
    "    for b in range(batch):\n",
    "        for d in range(num_box_priors):\n",
    "            conf_map = output[b, d * 25 + 4, :, :]\n",
    "            conf_map = torch.sigmoid(conf_map)  # convert to [0,1] score\n",
    "            detect = (conf_map > conf_thresh)  # binary tensors\n",
    "            positive_loc = torch.nonzero(detect)\n",
    "            for i in range(len(positive_loc)):\n",
    "                cx = positive_loc[i][0]  # NOT SURE OF THE INDEXING ORDER\n",
    "                cy = positive_loc[i][1]\n",
    "                logits = output[b, d * 25 + 5:d * 25 + 25, cx, cy]\n",
    "                # compute softmax function\n",
    "                cls_confs = torch.exp(logits) / torch.sum(torch.exp(logits))\n",
    "                # define most likely class\n",
    "                cls_conf, cls_idx = torch.max(cls_confs, 0)\n",
    "                if int(cls_idx[0])==6:  # we're only interested in cars\n",
    "                    # extract box parameters prediction\n",
    "                    tx = output[b, d * 25, cx, cy] # this is no longer a tensor. Float.\n",
    "                    ty = output[b, d * 25+1 , cx, cy]\n",
    "                    tw = output[b, d * 25 + 2, cx, cy]\n",
    "                    th = output[b, d * 25 + 3, cx, cy]\n",
    "                    detect_conf = output[b,d*25+4, cx, cy]\n",
    "                    # transform parameter into box centers and dimensions. See YOLO9000 for these formulas.\n",
    "                    # NOTE : we use cx to get y coord and vice-versa. This is due to the orientation of the axes in the\n",
    "                    # image: x is horizontal, pointing to the right, and y is vertical, pointing to the bottom. In a 2D\n",
    "                    # matrix, the first axis correspond to the rows, and the second to the columns. Hence, the\n",
    "                    # representaiton of the image is flipped wrt. the original and we need this to get the center coord. in the image axis.\n",
    "                    bx = sigmoid(tx) + cy\n",
    "                    by = sigmoid(ty) + cx\n",
    "                    bw = box_priors[2*d]*np.exp(tw)\n",
    "                    bh = box_priors[2*d+1]*np.exp(th)\n",
    "                    all_boxes.append([bx/w,by/h,bw/w,bh/h,detect_conf,cls_conf])\n",
    "\n",
    "    return all_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(boxes):\n",
    "    if len(boxes)==0:\n",
    "        return []\n",
    "    # sort boxes by order of confidence\n",
    "    det_confs = torch.zeros(len(boxes))\n",
    "    for i in range(len(boxes)):\n",
    "        det_confs[i] = boxes[i][4]\n",
    "    _, sort_idx = torch.sort(det_confs,descending=True)\n",
    "    # loop over the boxes\n",
    "    sorted = [boxes[sort_idx[i]] for i in range(len(boxes))]\n",
    "    remove=[]\n",
    "    for i in range(len(sorted)):\n",
    "        for j in range(i+1,len(sorted)):\n",
    "            if boxes_overlap(sorted[i],sorted[j])>0.4:\n",
    "                remove.append(j)\n",
    "    unique_boxes = [sorted[i] for i in range(len(sorted)) if i not in remove]\n",
    "    # loop over subsequent boxes\n",
    "    # remove any box that significantly overlap the first one\n",
    "    return unique_boxes\n",
    "\n",
    "def boxes_overlap(box1,box2):\n",
    "    # Here we compute a scalar indicating if two bounding boxes overlap significantly.\n",
    "    # The criteria is area of the intersection of the two boxes over area of the union of the two boxes\n",
    "    minx = min(box1[0] - box1[2] / 2.0, box2[0] - box2[2] / 2.0)\n",
    "    maxx = max(box1[0] + box1[2] / 2.0, box2[0] + box2[2] / 2.0)\n",
    "    miny = min(box1[1] - box1[3] / 2.0, box2[1] - box2[3] / 2.0)\n",
    "    maxy = max(box1[1] + box1[3] / 2.0, box2[1] + box2[3] / 2.0)\n",
    "    w1 = box1[2]\n",
    "    h1 = box1[3]\n",
    "    w2 = box2[2]\n",
    "    h2 = box2[3]\n",
    "    union_w = maxx - minx\n",
    "    union_h = maxy - miny\n",
    "    inter_w = w1 + w2 - union_w\n",
    "    inter_h = h1 + h2 - union_h\n",
    "    if inter_w <= 0 or inter_h <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    area1 = w1 * h1\n",
    "    area2 = w2 * h2\n",
    "    inter_area = inter_w * inter_h\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    return inter_area / union_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]  \n",
    "[2] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
